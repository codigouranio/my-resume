# Flask Configuration
HOST=0.0.0.0
PORT=5000
FLASK_ENV=production

# LLAMA Model Configuration (for app.py - local model)
LLAMA_MODEL_PATH=./models/llama-2-7b-chat.gguf

# CUDA Configuration (for RTX 3090)
CUDA_VISIBLE_DEVICES=0

# Remote LLAMA Server Configuration (for app_remote.py)
# Uncomment and configure if using an existing LLAMA server

# For llama.cpp server:
# LLAMA_SERVER_URL=http://localhost:8080
# LLAMA_API_TYPE=llama-cpp

# For Ollama:
# LLAMA_SERVER_URL=http://localhost:11434
# LLAMA_API_TYPE=ollama
# OLLAMA_MODEL=llama2

# For OpenAI-compatible API (LocalAI, vLLM, etc.):
# LLAMA_SERVER_URL=http://localhost:8000
# LLAMA_API_TYPE=openai
# MODEL_NAME=llama-2-7b-chat
